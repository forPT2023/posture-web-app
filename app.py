import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import os
from PIL import Image
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase

# Streamlit ã®è¨­å®š
st.set_page_config(page_title="å§¿å‹¢åˆ†æã‚¢ãƒ—ãƒª", layout="centered")
st.title("ğŸ“¸ ç«‹ä½å§¿å‹¢ï¼ˆçŸ¢çŠ¶é¢ï¼‰åˆ†æã‚¢ãƒ—ãƒª")
st.write("ã‚«ãƒ¡ãƒ©ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«å§¿å‹¢ã‚’è§£æã—ã¾ã™ï¼")

# å´é¢é¸æŠ
side_option = st.radio("å´é¢ã‚’é¸æŠ", ("å·¦å´é¢", "å³å´é¢"))
st.text(f"ç¾åœ¨ã®å´é¢: {side_option}")

# MediaPipe ã® Pose ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
mp_pose = mp.solutions.pose
pose = mp_pose.Pose()
mp_drawing = mp.solutions.drawing_utils

# çŸ¢çŠ¶é¢ç”¨ã®ãƒãƒ¼ã‚«ãƒ¼ã‚»ãƒƒãƒˆ
LEFT_SAGITTAL_MARKERS = [
    mp_pose.PoseLandmark.LEFT_EAR, mp_pose.PoseLandmark.LEFT_SHOULDER,
    mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.LEFT_KNEE,
    mp_pose.PoseLandmark.LEFT_ANKLE
]
RIGHT_SAGITTAL_MARKERS = [
    mp_pose.PoseLandmark.RIGHT_EAR, mp_pose.PoseLandmark.RIGHT_SHOULDER,
    mp_pose.PoseLandmark.RIGHT_HIP, mp_pose.PoseLandmark.RIGHT_KNEE,
    mp_pose.PoseLandmark.RIGHT_ANKLE
]

# ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
save_path = "captured_images"
os.makedirs(save_path, exist_ok=True)

# ç”»åƒã®åˆæœŸåŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³
clear_images = st.checkbox("ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã«å‰å›ã®ç”»åƒã‚’æ¶ˆå»ã™ã‚‹", value=True)
if clear_images:
    for file_name in ["before.png", "after.png", "comparison.png"]:
        file_path = os.path.join(save_path, file_name)
        if os.path.exists(file_path):
            os.remove(file_path)

# ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼æ©Ÿèƒ½
before_image_path = os.path.join(save_path, "before.png")
after_image_path = os.path.join(save_path, "after.png")
comparison_image_path = os.path.join(save_path, "comparison.png")

# WebRTC ã‚’ä½¿ã£ãŸã‚«ãƒ¡ãƒ©æ˜ åƒã®å–å¾—
class PoseEstimationTransformer(VideoTransformerBase):
    def __init__(self):
        self.pose = mp.solutions.pose.Pose()
    
    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.pose.process(img)
        height, width, _ = img.shape

        if results.pose_landmarks:
            selected_markers = LEFT_SAGITTAL_MARKERS if side_option == "å·¦å´é¢" else RIGHT_SAGITTAL_MARKERS
            points = {}
            for marker in selected_markers:
                lm = results.pose_landmarks.landmark[marker]
                cx, cy = int(lm.x * width), int(lm.y * height)
                points[marker] = (cx, cy)
                cv2.circle(img, (cx, cy), 12, (255, 255, 255), -1)  # ç™½ç¸
                cv2.circle(img, (cx, cy), 8, (255, 0, 0), -1)  # èµ¤ç‚¹

            for i in range(len(selected_markers) - 1):
                if selected_markers[i] in points and selected_markers[i + 1] in points:
                    cv2.line(img, points[selected_markers[i]], points[selected_markers[i + 1]], (173, 216, 230), 3)

        return cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

# WebRTC ã‚¹ãƒˆãƒªãƒ¼ãƒ 
webrtc_ctx = webrtc_streamer(
    key="posture-analysis",
    video_transformer_factory=PoseEstimationTransformer,
    async_transform=True
)

# æ’®å½±ãƒœã‚¿ãƒ³é…ç½®
col1, col2 = st.columns(2)
with col1:
    if st.button("ğŸ“¸ ãƒ“ãƒ•ã‚©ãƒ¼ã‚’æ’®å½±"):
        if webrtc_ctx.video_transformer:
            frame = webrtc_ctx.video_transformer.transform(webrtc_ctx.video_frame)
            Image.fromarray(frame).save(before_image_path)
            st.success("ãƒ“ãƒ•ã‚©ãƒ¼ç”»åƒã‚’æ’®å½±ã—ã¾ã—ãŸï¼")

with col2:
    if st.button("ğŸ“· ã‚¢ãƒ•ã‚¿ãƒ¼ã‚’æ’®å½±ï¼†æ¯”è¼ƒ"):
        if webrtc_ctx.video_transformer:
            frame = webrtc_ctx.video_transformer.transform(webrtc_ctx.video_frame)
            Image.fromarray(frame).save(after_image_path)
            st.success("ã‚¢ãƒ•ã‚¿ãƒ¼ç”»åƒã‚’æ’®å½±ã—ã¾ã—ãŸï¼")

# ç”»åƒè¡¨ç¤ºã¨ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
before_exists = os.path.exists(before_image_path)
after_exists = os.path.exists(after_image_path)
if before_exists:
    before_image = Image.open(before_image_path)
    st.image(before_image, caption="ãƒ“ãƒ•ã‚©ãƒ¼", use_container_width=True)
    st.download_button("ğŸ“¥ ãƒ“ãƒ•ã‚©ãƒ¼ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=open(before_image_path, "rb").read(), file_name="before.png", mime="image/png")

if after_exists:
    after_image = Image.open(after_image_path)
    st.image(after_image, caption="ã‚¢ãƒ•ã‚¿ãƒ¼", use_container_width=True)
    st.download_button("ğŸ“¥ ã‚¢ãƒ•ã‚¿ãƒ¼ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=open(after_image_path, "rb").read(), file_name="after.png", mime="image/png")

    if before_exists:
        before_np = np.array(before_image)
        after_np = np.array(after_image)

        if before_np.shape != after_np.shape:
            height = min(before_np.shape[0], after_np.shape[0])
            width = min(before_np.shape[1], after_np.shape[1])
            before_np = cv2.resize(before_np, (width, height))
            after_np = cv2.resize(after_np, (width, height))

        comparison_image = np.hstack((before_np, after_np))
        Image.fromarray(comparison_image).save(comparison_image_path)

        comparison_pil = Image.open(comparison_image_path)
        st.image(comparison_pil, caption="ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼æ¯”è¼ƒ", use_container_width=True)
        st.download_button("ğŸ“¥ ãƒ“ãƒ•ã‚©ãƒ¼ã‚¢ãƒ•ã‚¿ãƒ¼ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=open(comparison_image_path, "rb").read(), file_name="comparison.png", mime="image/png")

